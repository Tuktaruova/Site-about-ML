<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Обзор применений методов машинного обучения</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Обзор применений методов машинного обучения</h1>
    </header>
    <main>
        <article>
            <section class="logo">
                <img src="images/images-zDuzV16DL-transformed.png">
                <p>Определение машинного обучения от Oracle СНГ – «<strong>Машинное обучение (ML)</strong> — это направление искусственного интеллекта 
                    (ИИ), сосредоточенное на создании систем, которые обучаются и развиваются на основе получаемых ими данных. 
                    <strong>Искусственный интеллект</strong> — это широкий термин, который включает в себя компьютерные системы, имитирующие человеческий интеллект. Машинное обучение и ИИ 
                     часто идут бок о бок, и термины иногда используются взаимозаменяемо, но, строго говоря, это не одно и то же. Разница состоит в том, 
                    что машинное обучение всегда подразумевает использование ИИ, однако ИИ не всегда подразумевает машинное обучение.» </p>
            </section>
            <section class="fourthslide">
                <h3>История Машинного Обучения</h3>
                <p class="history">В 1959 году Артур Самуэль, исследователь искусственного интеллекта, ввел термин «машинное обучение». Он изобрел первую самообучающуюся компьютерную программу по игре в шашки. Самуэль определил машинное обучение как процесс, в результате которого компьютеры способны показать такое поведение, которое в них не было запрограммировано изначально.
                    <br><br>Ниже рассмотрим другие важные даты в истории машинного обучения:
                    <br> <strong>1946:</strong>  появился компьютер ЭНИАК — сверхсекретный проект армии США.
                    <br><strong>1950:</strong> Алан Тьюринг создает “Тьюринг тест” для оценки интеллекта компьютера.
                    <br><strong>1958:</strong> Фрэнк Розенблатт придумал Персептрон — первую искусственную нейронную сеть и создал первый нейрокомпьютер «Марк-1».
                    <br><strong>1959:</strong> Марвин Минский создал первую машину SNARC со случайно связанной нейросетью.
                    <br><strong>1967:</strong> Написан метрический алгоритм по классификации данных. Алгоритм позволил компьютерам применять простые шаблоны распознавания.
                    <br><strong>1997:</strong> Компьютер Deep Blue обыграл чемпиона мира, Гарри Каспарова, в шахматы.
                    <br><strong>2006:</strong> Джеффри Хинтон, ученый в области искусственных нейросетей, ввел термин «Глубинное обучение» (Deep learning).
                    <br><strong>2012:</strong> В Google X Lab разработали алгоритм, позволяющий идентифицировать видеоролики, в которых показываются коты.
                    <br><strong>2014:</strong> В Facebook изобрели DeepFace для распознавания лиц. Точность алгоритма 97%.
                    <br><strong>2015:</strong> Amazon запустила собственную платформу машинного обучения — Amazon Machine Learning.
                    <br><strong>2020:</strong> Технологии искусственного интеллекта применяются практически в каждом программном продукте.Как связаны машинное и глубокое обучение, ИИ и нейросети
                    </p>
            </section>
            <section class="sposops">
                <div class="wrapper">
                    <img src="images/classic-transformed.jpeg">
                    <img src="images/scheme.jpeg">
                </div>
                
                <h3>Способы машинного обучения</h3>
                <p>
                    В мире машинного обучения существуют три основных подхода: обучение с учителем, обучение без учителя и обучение с подкреплением. Отдельно можно выделить блок нейросетей и глубокого обучения. Также существуют ансамблевые методы.
                </p>
            </section>
            <section class="supervisedlearning">
                <h3>Обучение с учителем (supervised learning)</h3>
                <p>
                    <br>Суть этого вида машинного обучния в том, что для обучения нейросеть получает специальный набор данных (датасет), в котором заранее отмечено, что эти данные означают. То есть нейросеть получает и вопрос, и ответ, 
                    который должна давать. Она «смотрит» на большие объёмы таких данных и так учится отвечать правильно.
                    <p class="parag"><br>Чтобы такое обучение было возможным, нужно предварительно собрать, отсмотреть вручную и разметить обучающий датасет. Этим должен заниматься человек — как раз тот самый условный «учитель» (supervisor). 
                    <br>В области анализа данных, обучение с учителем применяется для задач классификации и регрессии. В случае классификации целевой переменной является метка класса, а в регрессии - числовая переменная. 
                    <br>Задача классификации в машинном обучении — это задача отнесения объекта к одному из заранее определенных классов на основании его формализованных признаков. Каждый из объектов в этой задаче представляется в виде вектора 
                    в N-мерном пространстве, каждое измерение в котором представляет собой описание одного из признаков объекта. Допустим нам нужно классифицировать мониторы: измерениями в нашем пространстве параметров будут величина диагонали 
                    в дюймах, соотношение сторон, максимальное разрешение, наличие HDMI-интерфейса, стоимость и др.
                    <br>Для обучения классификатора необходимо иметь набор объектов, для которых заранее определены классы. Это множество называется обучающей выборкой, её разметка производится вручную, с привлечением специалистов в исследуемой области. 
                    В задаче классификации может быть более двух классов (многоклассовая), каждый из объектов может принадлежать более чем к одному классу (пересекающаяся).
                    <br>У классификации есть полезная обратная сторона — поиск аномалий. Когда какой-то признак объекта сильно не вписывается в наши классы, мы ярко подсвечиваем его на экране. Сейчас так делают в медицине: компьютер подсвечивает врачу все подозрительные области МРТ или выделяет отклонения в анализах. На биржах таким же образом определяют нестандартных игроков, которые скорее всего являются инсайдерами.
                    Научив компьютер «как правильно», мы автоматически получаем и обратный классификатор — как неправильно.</p>
                </p>
                    <p class="Kmeans">
                        <strong>Метод К-ближайших соседей (K-NN)</strong>  – это один из самых простых алгоритмов классификации, также иногда используемый в задачах регрессии.
                        <br>Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:
                        <br>Вычислить расстояние до каждого из объектов обучающей выборки
                        <br>Отобрать k объектов обучающей выборки, расстояние до которых минимально
                        <br>Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей
                        <strong><h4>Преимущества и недостатки KNN</h4></strong>  
                        <strong>Преимущества:</strong> 
                        • простота в реализации и интерпретации;
                        •	применяется во многих задачах, особенно в рекомендательных системах;
                        •	высокая точность прогнозов при правильном подборе k и метрики расстояния.
                        <strong>Недостатки:</strong>
                        •	большое потребление памяти и низкая скорость работы из-за хранения и вычисления расстояний между всеми обучающими и тестовыми образцами (имеется в виду KNN в чистом виде);
                        •	чувствительность к выбросам и шуму, а также к несбалансированным классам в данных;
                        •	при большом количестве признаков может возникнуть проблема совпадения метрической и смысловой близости объектов, что решается с помощью обучения представлений (численное описание объектов).
                        <br><br><strong>Области применения алгоритма</strong>
                        Алгоритм KNN может применяться практически во всех задачах классификации, особенно в тех случаях, когда оценить параметры вероятностного распределения данных сложно или невозможно. 
                         Наиболее типичными приложениями алгоритма KNN являются:
                        <br>классификация клиентов (например, по уровню лояльности);
                        <br>медицина — классификация пациентов по медицинским показателям;
                        <br>маркетинг — классификация товаров по уровню популярности и т.д.
                    </p>
                    <p class="NaiveBayes">
                        <strong>Наивный байесовский классификатор (Naive Bayes classifier)</strong> — вероятностный классификатор на основе формулы Байеса со строгим предположением о независимости признаков между собой при заданном классе, что сильно упрощает
                         задачу классификации из-за оценки одномерных вероятностных плотностей вместо одной многомерной.
                        <br>Это алгоритм, предназначенный для многоклассовой классификации данных с независимыми признаками. За один проход вычисляется условная вероятность каждого признака, затем применяется теорема Байеса для нахождения
                         распределения вероятности наблюдений. 
                        <br>Часто применяют для классификации текстов, например, для фильтрации спама. Допустим, есть датасет, где каждое наблюдение — это тип текста, а признаки — это слова, из которых он состоит. Значениями этих признаков 
                        могут быть частота слова или значения 0/1, указывающие есть ли данное слово в тексте или нет. 
                    </p>
                    <p class="SVM">
                        <strong>Метод опорных векторов (англ. SVM, Support Vector Machine)</strong>
                        <br>Этот метод можно использовать для классификации видов растений, лица на фотографиях, документов по тематикам и т.д.
                        <br>Идея SVM проста — он ищет, как так провести две прямые между категориями, чтобы между ними образовался наибольший зазор. На картинке нагляднее:
                    </p>
                    <p class="Tree">
                        <strong>Дерево решений</strong> —инструмент представляет собой иерархическую древовидную структуру, состоящую из правила вида «Если …, то ...».
                        За счет обучающего множества правила генерируются автоматически в процессе обучения.
                        <br>Включает в себя элементы двух типов — узлов (node) и листьев (leaf). Узлы включают в себя решающие правила и производят проверку примеров на соответствие выбранного атрибута обучающего множества.
                        <br>Простой случай: примеры попадают в узел, проходят проверку и разбиваются на два подмножества:
                        <br>первое — те, которые удовлетворяют установленное правило;
                        <br>второе — те, которые не удовлетворяют установленное правило.
                        <br>Процедура повторяется, пока не будет достигнуто условие остановки алгоритма. Последний узел становится листом.
                        <br>В листе содержится не правило, а подмножество объектов, удовлетворяющих всем правилам ветви, которая заканчивается этим листом. Пример попадает в лист, если соответствует всем правилам на пути к нему. 
                        К каждому листу есть только один путь, что обеспечивает единственность решения
                        <br><strong><h4>Преимущества и недостатки дерева решений</h4></strong>
                         <br><strong>Преимущества:</strong>
                        <br>•	Формируют четкие и понятные правила классификации. Например, «если возраст меньше 40, то отказать в кредите». То есть деревья решений хорошо и быстро интерпретируются.
                        <br>•	Способны генерировать правила в областях, где специалисту трудно формализовать свои знания.
                        <br>•	Могут «интерпретироваться» не только как модель в целом, но и как прогноз для отдельного тестового субъекта.
                        <br>•	Быстро обучаются и прогнозируют.
                        <br>•	Не требуется много параметров модели.
                        <br>•	Поддерживают как числовые, так и категориальные признаки.
                        <br><strong>Недостатки:</strong>
                        <br>•	Деревья решений чувствительны к шумам во входных данных. 
                        <br>•	Разделяющая граница имеет определенные ограничения, из-за чего дерево решений по качеству классификации уступает другим методам.
                        <br><strong>Успешнее всего деревья применяют в следующих областях:</strong>
                        <br>•	Банковское дело. Оценка кредитоспособности клиентов банка при выдаче кредитов.
                        <br>•	Промышленность. Контроль качества продукции (обнаружение дефектов в готовых товарах), испытания без нарушений (например, проверка качества сварки) и т.п.
                        <br>•	Медицина. Диагностика заболеваний разной сложности.
                        <br> •	Молекулярная биология. Анализ строения аминокислот.
                        <br>•	Торговля. Классификация клиентов и товара.
                        <p class="Regression">
                            <strong>Регрессия (regression)</strong> - откликом модели является действительное число или числовой вектор. Стоимость автомобиля по его пробегу, количество пробок по времени суток, объем спроса на товар от роста 
                            компании и т.д. На регрессию идеально ложатся любые задачи, где есть зависимость от времени.
                            <br>Модель пытается нарисовать линию, которая в среднем отражает зависимость считая среднее расстояние до каждой точки.
                            <br>Сегодня используют для:
                            <br>•	Прогноз стоимости ценных бумаг
                            <br>•  	Анализ спроса, объема продаж
                            <br>•	Медицинские диагнозы
                            <br>•	Любые зависимости числа от времени
                            <br>Когда регрессия рисует прямую линию, её называют линейной, когда кривую — полиномиальной.8
                        </p>
                    </p>
            </section>
            <section class="unsupervised">
                <h3>Обучение без учителя (unsupervised learning)</h3>
                <p> Данный алгоритм самостоятельно выявляет закономерности в данных. Этот подход применяется для кластеризации похожих текстов или изображений.
                <br>Обучение без учителя отличается от обучения с учителем тем, что в этом случае система обучается на неразмеченных данных, данные не содержат целевых значений или меток классов. Алгоритмы обучения без учителя стремятся найти скрытые 
                структуры или паттерны в данных, чтобы выделить группы или кластеры объектов, обнаружить аномалии, выполнить снижение размерности и другие задачи.
                <br>Примеры задач, которые могут быть решены с помощью обучения без учителя, включают кластеризацию, понижение размерности, поиск правил и т. д.
                </p>
                <p class="clasterisation">
                    Кластеризация — это набор методов для группировки данных по определённым критериям в кластеры, что позволяет выявлять сходства и различия между объектами, а также упрощать их анализ и визуализацию. Из-за частичного сходства 
                    в постановке задач с классификацией кластеризацию ещё называют unsupervised classification.
                    <br>Области применения кластеризации и её разновидности
                    <br>o	классификация (определение к какому классу относится каждый объект);
                    <br>o	сегментация рынка (разделение клиентов на группы по их характеристикам для разработки эффективных стратегий в маркетинге и продажах);
                    <br>o	сегментация изображений (разделение изображения на сегменты или группы пикселей);
                    <br>o	кластеризация геоданных (группировка данных по их географическому расположению, например, разделение районов на безопасные и опасные, богатые и бедные);
                    <br>o	понижение размерности (уменьшение количества признаков путем объединения схожих в один кластер). 
                </p>
                <P class="kmeans">
                    Расскажу про некоторые методы кластеризации.
                    <br><strong>Алгоритм k-средних (k-means)</strong>
                    <br>Буква k в названии отвечает за количество выделенных центроидов — точек, которые формируют вокруг себя кластер. 
                    <br>Возьмем значение k, равное 5. Для каждой точки датасета посчитаем расстояние до каждого из пяти кластеров. Наименьшее расстояние от точки до центроида позволит модели предположить, что объект принадлежит к этому кластеру.
                    <br>После формирования кластеров координаты центроида должны быть пересчитаны, так как центральный элемент кластера может измениться после добавления элементов. Будем пересчитывать центры кластеров пока координаты центроида не 
                    перестанут меняться и представлять усредненное описание всех параметров кластера.
                    <br>У алгоритма есть ограничение: чаще всего не знаем, сколько кластеров предполагается. Можно последовательно брать разные k: 3, 5, 7, 9 и строить график. По нему мы можем понять, какое k оптимальнее взять для задачи. 
                    До точки k = 3 наблюдается быстрое уменьшение отклонения, но потом скорость падения снижается, откуда мы делаем вывод, что k = 3 было точкой оптимума.
                </P>
                <p class="DBSCAN">
                    <strong>DBSCAN</strong>
                    <br>Алгоритмы кластеризации на основе плотности – это один из подходов при группировке пространственных данных. Основное отличие плотностных алгоритмов от разделительных – возможность определять кластеры произвольной формы. 
                    Плотностные алгоритмы сами определяют необходимое количество кластеров. 
                    <br>Принцип работы плотностных алгоритмов – это выделение областей высокой концентрации точек, которые разделяются областями с низкой плотностью. Точки, которые попали в области с низкой плотностью, определяются как выбросы.
                    <br>Кластеризация на основе плотности – наиболее грамотный алгоритм для пространственных данных. Для того, чтобы выявить области с наибольшим количеством ДТП или определить области заболеваний, вероятнее всего прибегнут к плотностным алгоритмам.
                </p>
                <p class="fuzzyc">
                    <br><strong>Нечеткий алгоритм кластеризации с-means (Fuzzy C-means clustering)</strong> - Вместо однозначного ответа на вопрос к какому кластеру относится объект, он 
                    определяет вероятность того, что объект принадлежит к тому или иному кластеру. Таким образом, утверждение «объект А принадлежит к кластеру 1 с 
                    вероятностью 90%, к кластеру 2 — 10% » верно и более удобно.  Этот алгоритм похож на алгоритм k-means. Обобщенное описание работы алгоритма: выбор количества кластеров; Случайным образом присвоить коэффициенты каждой точке данных за то, что она находится в кластерах; повторять до тех пор, пока алгоритм не достигнет сходимости (то есть изменение коэффициентов между двумя итерациями не превысит заданный порог чувствительности) .
	                <br>Fuzzy c-means – важный инструмент для обработки изображений при кластеризации объектов на изображении. В биоинформатике этот метод кластеризации 
                    используется для анализа данных об экспрессии генов с помощью данных РНК-секвенирования. В маркетинге клиенты могут быть сгруппированы в нечеткие 
                    кластеры на основе их потребностей, выбора бренда, психологических профилей или других разделов, связанных с маркетингом.
                </p>
                <p class="asssosiations">
                    <br><strong>Поиск правил (ассоциаций)</strong> - В этой задаче модель ищет взаимосвязи между существующими объектами.
                    <br>Классический пример — взаимосвязь продуктов в корзине покупателя. Например, к молоку и яйцам рекомендательная система магазина может посоветовать взять муку. Чтобы получить такой «совет», нужно проанализировать, какие комбинации продуктов встречаются чаще всего.
                    <br><strong>Алгоритм Apriori</strong> - Он находит самые частые комбинации, и модель создает правила ассоциативности: когда встречаются два продукта, к ним присоединяется третий.
                    <br>Для работы с алгоритмом нужно знать такие понятия.
                    <br>Support. Показывает минимальный порог встречаемости элемента, чтобы считаться частым.
                    <br>Confidence. Как часто выделенные взаимосвязи встречаются во всех объектах во всем датасете.
                    <br>Lift. Зависимость между объектами.
                    <br>Conviction. Как часто утверждается, что товары связаны, хотя на самом деле взаимосвязи нет.
                    <br><br><strong>Уменьшение размерности</strong>
                   <br>Данные могут быть слишком взаимосвязаны между собой. В таком случае можно снизить размерность, заменив два признака на один. Особенно это
                    актуально в больших данных, когда нужно сократить датасет и обработать меньшее количество данных.
                    <br>Для этой задачи используется алгоритм <strong>PCA</strong>. Собственно, он как раз и «жертвует» каким-то некритичным количеством информации, подбирая новый
                    признак, который будет включать в себя свойства обоих изначальных признаков.
                </p> 
            <section class="reinforcementlearning">
                <h3>Обучение с подкреплением (reinforcement learning)</h3>
                    <p><strong>Обучение с подкреплением</strong> представляет собой процесс, в котором модель принимает решения в изменяющейся среде с целью 
                    максимизации награды. Например, алгоритм обучает космический корабль совершать успешные посадки в различных условиях, адаптируя свои действия на основе полученной обратной связи</p>
                    <p>
                    <br>То есть происходит обучение модели, которая не имеет сведений о системе, но имеет возможность производить какие-либо действия в ней. Действия переводят систему в новое состояние и модель получает от системы некоторое вознаграждение. 
                    <br>Следует сделать некоторые уточнения: награда не подсказывает, как именно нужно решать задачу и что вообще нужно делать, награда может быть отложенной во времени или сильно разреженной. Всё это сильно отличает задачу от обучения с учителем. Награда предоставляет какой-то «сигнал» для обучения (хорошо/плохо), которого нет, например, в обучении без учителя.
                    <br>Обучение с подкреплением используется в следующих сферах:
                    <br>Робототехника для промышленной автоматизации (например, конвейерная сборка).
                    <br>Планирование бизнес-стратегии.
                    <br>Автоматизация внутри самого машинного обучения.
                    <br>Продвинутые система рекомендации, например, на университетских ресурсах для дополнительного обучения студентов.
                    <br>Управление движением робота, автопилот.
                    <br><strong>Алгоритмы обучения с подкреплением</strong> 
                    <br><strong>State-Action-Reward-State-Action (SARSA).</strong> Этот алгоритм обучения с подкреплением начинается с предоставления агенту такого коэффициента, как 
                    политика (on-policy). В данном случае политика – это вероятность, с помощью которой алгоритм оценивает шансы определенных действий, приводящих к 
                    вознаграждениям или положительным состояниям.
                    <br><strong>Q-Learning.</strong>В этом подходе к RL используется противоположный подход. Агент не получает on-policy, соответственно, его исследование окружающей 
                    среды является более самостоятельным. В Q-learning у нас нет ограничений на выбор действия (action) для алгоритма. Он полагает, что все последующие actions будут оптимальными по умолчанию, поэтому алгоритм производит операцию выбора исходя из максимизации оценки Q.
                    <br><strong>Deep Q-Networks (Глубокие Q-сети).</strong> Этот алгоритм использует нейронные сети в дополнение к методам обучения с подкреплением. Нейросети 
                    осуществляют самостоятельное исследование (research) среды обучения с подкреплением для выбора наиболее оптимального значения. То, как алгоритм будет себя вести и подбирать значения, основано на выборке прошлых положительных действий, полученных нейронной сетью
                    </p>
            </section>
            <section class="ansamble">
                <h3>Ансамблевые методы</h3>
                <p class="am">
                    <br><strong>Ансамблевый метод </strong> — метод машинного обучения, где несколько моделей обучаются для решения одной и той же проблемы и объединяются для получения
                     лучших результатов. Ансамбли позволяют увеличить точность модели до 90+, при этом они довольно просты в понимании. 
                    <br>Вводится понятие слабого ученика (обычные модели вроде линейной регрессии или дерева решений). Множество слабых учеников являются строительными 
                    блоками для более сложных моделей. Объединение слабых учеников для улучшения качества модели, уменьшения смещения или разброса, называется сильным учеником.
                    <br>Наиболее популярными ансамблевыми методами являются: стекинг, бэггинг, бустинг.
                    <br><strong>Стекинг.</strong> Используется несколько разнородных слабых учеников. Их обучают и объединяют для построения прогноза, основанного на результатах различных слабых моделей.
                    <br><strong>Бэггинг.</strong> В этом случае однородные модели обучают на разных наборах данных и объединяют. Получают прогноз путём усреднения. Если использовать в 
                    качестве слабого ученика деревья решений, то получится случайный лес RandomForestClassifier / RandomForestRegressor.
                    <br><strong>Бустинг.</strong> При использовании данного метода несколько однородных моделей последовательно обучаются, исправляя ошибки друг друга.
                    <br>Подробнее расскажу про бустинг. Берётся множество одинаковых моделей и объединяется, чтобы получить сильного ученика. Модели приспосабливаются к данным последовательно, каждая модель будет исправлять ошибки предыдущей.
                    <br>Базовые модели для бустинга — это модели с низким разбросом и высоким смещением. Например, неглубокие деревья решений. Одна из причин такого выбора моделей - они требуют меньше вычислительных затрат. 
                    <br>Существует два наиболее распространённых алгоритма бустинга - адаптивный бустинг и градиентный бустинг. 
                </p>
            </section>     
            <section>
                <h3>Нейросети и глубокое обучение</h3>
                <p>Следующий большой блок машинного обучения - глубокое обучение и нейросети</p>
                <p class="DLdescription">
                    <br><strong>Глубокое обучение (Deep Learning, DL)</strong> — тип машинного обучения, задействующий искусственные нейронные сети, моделирующие аналитические действия 
                    человеческого мозга, чтобы научить цифровые системы самообучаться и принимать решения на основе неструктурированных неразмеченных данных.
                    <br>Процесс обучения называется глубоким, так как структура искусственных нейронных сетей состоит из нескольких входных, выходных и скрытых слоев. Каждый слой содержит единицы, преобразующие входные данные в сведения, которые следующий слой может использовать для определенной задачи прогнозирования. Благодаря этой структуре компьютер может обучаться с помощью собственной обработки данных.
                    <br>Результативность DL определяется работой нейросетей, которые выявляют закономерности, придерживаются входящих правил, создают знания на основе 
                    примеров и имитируют человеческие реакции.
                    <br>Основа DL — входящие данные, которые помогают алгоритмам найти и запомнить общие признаки для их последующего выявления на необработанных данных.
                     При совершении ошибок алгоритм получает штраф, представленный с точки зрения математики корректировкой работы функции.
                    <br>Обучение сводится к нескольким этапам:
                    <br>•	загрузке массива данных;
                    <br>•	выявлению признаков и подготовке ответа;
                    <br>•	проверке ответа на соответствие;
                    <br>•	завершению обучения или перенастройке сети и повторению цикла обучения.
                    <br>Результат каждой последующей попытки становится ближе к правильному ответу: на точность влияет объем исходных данных, продолжительность обучения объем задействованных для этого вычислительных мощностей.
                    <br><strong>Примеры использования глубокого обучения</strong>
                    <br><strong>Классификация</strong>
                    <br>DL используют для отделения целых компонентов от поврежденных и их последующей сортировки по типам повреждений. В производстве матриц для ноутбуков классификация будет заключаться в проверке каждого изделия на целостность и разделения изображений по характеру дефектов.
                    <br><strong>Сегментация изображений и распознавание объектов</strong>
                    <br>Сегментация подразумевает определение класса каждого пикселя изображения. Такой подход помогает алгоритмам различать объекты даже на больших и заполненных изображениях — находить на картинках дорожные знаки указанной формы, автомобили, здания. Например, DL может распознать объекты на конвейерной ленте и отнести их к той или иной группе.
                    <br><strong>Обработка изображений</strong>
                    <br>Метод может использоваться для обработки и оптимизации фотографий и видеофайлов. DL помогает убирать с кадров шум, компенсировать искажения, восстанавливать поврежденные или неудавшиеся участки изображения и других задач. 
                    <br>Рассмотрим некоторые методы DL.
                </p>
                <p class="MLPCNN">
                    <br><strong>Многослойный перцептрон (MLP)</strong> - Для обучения MLP используется принцип обратного распространения ошибки. Одно из преимуществ MLP — способность 
                    обучаться на набор  ах данных любого размера. В чистом виде MLP уже не используется для решения практических задач в области ML, но применяется в 
                    составе более сложных нейросетей.
                    <br>MLP применялся для анализа разных типов данных — медицинских, финансовых, технических, позволяя прогнозировать состояние сложных систем. MLP даже использовали в ранних версиях таких приложений ИИ, как виртуальные помощники, сервисы распознавания рукописного текста и транскрибации речи. 
                    <br><strong>Сверточная нейронная сеть (CNN)</strong> — это эффективная искусственная нейронная сеть, имеющая уникальную архитектуру. Слои в ней организованы в 
                    трех измерениях: ширина, высота и глубина. Нейроны в одном слое соединяются не со всеми нейронами в следующем слое, а только с небольшой областью нейронов этого слоя. Окончательный результат сокращается до одного вектора оценки вероятности, упорядоченного по глубине в одном из измерений.
                    <br>Сверточные нейронные сети используются в таких областях, как распознавание видео, распознавание изображений и в системах выработки рекомендаций
                </p>
            </section>           
            </section>
        </article>
    </main>
</body>
<footer>Сайт сделан для предмета "Введение в профессию" Выполнила Туктарева Анастасия Евгеньевна, группа ИБ31з</footer>
</html>